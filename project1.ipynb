{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'data/'\n",
    "training_data = data_folder + 'train.csv'\n",
    "testing_data = data_folder + 'test.csv'\n",
    "\n",
    "methods = ['mse', 'mae', 'log', 'ridge-regression']\n",
    "method = methods[3]\n",
    "\n",
    "pos_weight = 1\n",
    "lambda_1 = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We map the 'b' and 's' labels to 0 and 1 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    Y = np.genfromtxt(training_data, delimiter=',', dtype=None, skip_header=1, usecols=[1], converters={1: lambda x: 0 if b'b'==x else 1})\n",
    "    \n",
    "    data = np.genfromtxt(training_data, delimiter=',', skip_header=1)\n",
    "    X = data[:, 2:]\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and standardize the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting the number of invalid datapoints per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalids = np.count_nonzero(X == -999, axis=0)\n",
    "print(invalids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore we get rid of columns [0,4,5,6,12,23,24,25,26,27,28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_standardize_features(X):\n",
    "    X_clean = np.delete(X,[0,4,5,6,12,23,24,25,26,27,28], axis=1)\n",
    "    X_standardized = (X_clean - X_clean.mean(axis=0))/X_clean.std(axis = 0)\n",
    "    X_standardized = np.insert(X_standardized, 0, 1, axis=1)\n",
    "    return X_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_standardized = clean_and_standardize_features(X)\n",
    "N = X_standardized.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = np.corrcoef(X_standardized[:10000,:], rowvar=False)\n",
    "suffix = time.time()\n",
    "np.savetxt('correlation_matrix_' + str(suffix) + '.csv', correlation_matrix, fmt=\"%0.2f\", delimiter=\",\", comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_heatmap(train):\n",
    "    correlations = np.corrcoef(X_standardized[:15000,:], rowvar=False)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    sns.heatmap(correlations, vmax=1.0, center=0, fmt='.2f',\n",
    "                square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .70})\n",
    "    plt.show();\n",
    "    \n",
    "correlation_heatmap(X_standardized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods from lab1 and lab2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    predictions = tx@w\n",
    "    error = y-predictions\n",
    "    if(method == 'mse'):\n",
    "        return 1/(2*y.shape[0])*np.sum(error*error)\n",
    "    elif(method == 'mae'):\n",
    "        return 1/(2*y.shape[0])*np.sum(np.abs(error))\n",
    "    elif(method == 'log'):\n",
    "        predictions = predict(tx, w)\n",
    "        return -np.sum(y*np.log(predictions)*pos_weight + (1-y)*np.log(1-predictions))/y.shape[0]\n",
    "    elif(method == 'ridge-regression'):\n",
    "        return 1/(2*y.shape[0])*np.sum(error*error)+ lambda_1*np.linalg.norm(w)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(y, tx, w0, w1):\n",
    "    losses = np.zeros((len(w0), len(w1)))\n",
    "    for i in range(len(w0)):\n",
    "        for j in range(len(w1)):\n",
    "            losses[i][j] = compute_loss(y, tx, np.array([w0[i], w1[j]]))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    if(method=='log'):\n",
    "        prediction = predict(tx, w)\n",
    "    else:\n",
    "        prediction = tx@w\n",
    "    error = y-prediction\n",
    "    gradient = -1/y.shape[0]*tx.T@error\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    i = 0\n",
    "    initial_w = initial_w/100 # initialize at small weights else gradient descent might explode at first iteration\n",
    "    w_res = initial_w\n",
    "    loss_hist = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        gradient = compute_gradient(y, tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        w = w - gamma * gradient\n",
    "        # store w and loss\n",
    "        w_res = w\n",
    "        loss_hist.append(loss)\n",
    "        #print(\"Gradient Descent({bi}/{ti}): loss={l}\".format(\n",
    "              #bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "        # Log Progress\n",
    "        i = i + 1\n",
    "        if i % 1000 == 0:\n",
    "            print(\"iter: \" + str(i) + \" loss: \"+str(loss_hist[-1]))\n",
    "\n",
    "    return loss_hist, w_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    w_res = initial_w\n",
    "    loss_res = 0\n",
    "    w = initial_w\n",
    "    ti = max_iters\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32, max_iters):\n",
    "        gradient = compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "        loss = compute_loss(minibatch_y, minibatch_tx, w)\n",
    "        w = w- gamma * gradient\n",
    "        # store w and loss\n",
    "        w_res = w\n",
    "        loss_res = loss\n",
    "    return loss_res, w_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use the sigmoid function: $$S(z) = \\frac{1}{1 + e^{-z}}$$ <br />\n",
    "to map the predicted values to probabilities of the event being a signal(1) rather than background(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + math.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_vec = np.vectorize(sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w):\n",
    "    temp = x@w\n",
    "    return sigmoid_vec(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the cross-entropy cost function for loss computation: \n",
    "$$J(\\theta) = -\\frac{1}{N} * (y^T log(sigmoid(Xw)) + (1-y)^T log(1-sigmoid(Xw)))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find where sigmoid goes to 1\n",
    "\n",
    "max_iters = 200\n",
    "gamma = 0.3\n",
    "batch_size = 1\n",
    "\n",
    "method = methods[2]\n",
    "losses, w = gradient_descent(Y, X_standardized, np.random.random(X_standardized.shape[1])/100, max_iters, gamma)\n",
    "print(\"w : \")\n",
    "print(w)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We label the results <0.5 to -1, and the rest to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the threshold to 0.618 for mse, 0.458 for cross-entropy\n",
    "def label_results(Y_predicted, threshold=0.61):\n",
    "    f = lambda x: -1 if x<threshold else 1\n",
    "    f_vec = np.vectorize(f)\n",
    "    return f_vec(Y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(Y, Y_predicted):\n",
    "    return np.sum(Y == Y_predicted)/Y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance(Y, Y_predicted):\n",
    "    false_negatives = 0\n",
    "    true_negatives = 0\n",
    "    false_positives = 0\n",
    "    true_positives = 0\n",
    "    for i in range(Y.shape[0]):\n",
    "        if(Y[i]==-1):\n",
    "            if(Y_predicted[i]==-1):\n",
    "                true_negatives += 1\n",
    "            else:\n",
    "                false_positives += 1\n",
    "        else:\n",
    "            if(Y_predicted[i]==-1):\n",
    "                false_negatives += 1\n",
    "            else:\n",
    "                true_positives += 1\n",
    "    #print('True positives: ' + str(true_positives))\n",
    "    #print('False positives: ' + str(false_positives))\n",
    "    #print('True negatives: ' + str(true_negatives))\n",
    "    #print('False negatives: ' + str(false_negatives))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_threshold(Y_predicted, Y_labeled):\n",
    "    best_t = 0.5\n",
    "    best_perf = performance(Y_labeled, label_results(Y_predicted, threshold=best_t))\n",
    "    for i in range(0, 100):\n",
    "        perf = performance(Y_labeled, label_results(Y_predicted, threshold=i/100))\n",
    "        #print('threshold is: ' + str(i/100))\n",
    "        #print('performance is: ' + str(perf))\n",
    "        if(perf>best_perf):\n",
    "            best_t = i/100\n",
    "            best_perf = perf\n",
    "    print('best threshold is: ' + str(best_t))\n",
    "    print('best performance is: ' + str(best_perf))\n",
    "    return best_t, best_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying the $L_2$ - regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_prime = lambda_1 * 2 * N\n",
    "temp = np.linalg.inv(X_standardized.T@X_standardized + np.identity(N)*lambda_prime)\n",
    "temp_2 = temp@X_standardized.T\n",
    "w_L_2 = temp_2@Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predicted = predict(X_standardized, w_L_2)\n",
    "Y_labeled = label_results(Y_predicted)\n",
    "print(performance(Y_labeled, label_results(Y)))\n",
    "# 0.735 for mse\n",
    "#0.74038 for cross-entropy\n",
    "evaluate_performance(label_results(Y), Y_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_threshold(Y_predicted, label_results(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying the ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 1000\n",
    "gamma = 0.15\n",
    "batch_size = 1\n",
    "\n",
    "method = methods[3]\n",
    "losses, w_ridge = gradient_descent(Y, X_standardized, np.ones(X_standardized.shape[1]), max_iters, gamma)\n",
    "print(w_ridge)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predicted = predict(X_standardized, w_ridge)\n",
    "Y_labeled = label_results(Y_predicted)\n",
    "print(performance(Y_labeled, label_results(Y)))\n",
    "# 0.735 for mse\n",
    "#0.74038 for cross-entropy\n",
    "evaluate_performance(label_results(Y), Y_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_threshold(Y_predicted, label_results(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 100\n",
    "gamma = 1\n",
    "batch_size = 1\n",
    "\n",
    "method = methods[2]\n",
    "losses, w = gradient_descent(Y, X_standardized, np.random.random(X_standardized.shape[1]), max_iters, gamma)\n",
    "print(w)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare different values of gamma for gradient descent\n",
    "max_iters = 600\n",
    "gamma = [0.5, 0.1, 0.15, 0.2, 0.3, 1]\n",
    "batch_size = 1\n",
    "whichLoss = 100\n",
    "which = 100\n",
    "losses = np.ndarray([len(gamma),1])\n",
    "w = np.ndarray([len(gamma), X_standardized.shape[1]])\n",
    "for i in range(len(gamma)):\n",
    "    loss, wi = gradient_descent(Y, X_standardized, np.random.random(X_standardized.shape[1]), max_iters, gamma[i]) \n",
    "    plt.tight_layout()\n",
    "    plt.subplot(int(str(32)+str(i+1)))\n",
    "    plt.plot(loss)\n",
    "    plt.xlabel(\"# of iterations\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.title(\"Gamma = \" + str(gamma[i]))\n",
    "    print(loss[-1])\n",
    "    if(loss[-1] < whichLoss):\n",
    "        whichLoss = loss[-1]\n",
    "        which = i\n",
    "    losses[i] = loss[-1]\n",
    "    w[i,:] = wi\n",
    "\n",
    "print(\"smallest cost:\" + str(losses[which]) + \"at gamma=\" + str(gamma[which]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    Y_predicted = predict(X_standardized, w)\n",
    "except:\n",
    "    Y_predicted = predict(X_standardized, w[which,:])\n",
    "Y_labeled = label_results(Y_predicted, 0.46)\n",
    "print(performance(Y_labeled, label_results(Y)))\n",
    "# 0.735 for mse\n",
    "#0.74038 for cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_performance(label_results(Y), Y_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_threshold(Y_predicted, label_results(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oversampling the signal class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = np.column_stack([Y, X_standardized])\n",
    "distrib = np.bincount(all_data[:,0].astype(int))\n",
    "prob = 1/distrib[all_data[:, 0].astype(int)].astype(float)\n",
    "prob /= prob.sum()\n",
    "all_data = all_data[np.random.choice(np.arange(len(all_data)), size=np.count_nonzero(distrib)*distrib.max(), p=prob)]\n",
    "print(all_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rerunning the gradient descent with oversampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 500\n",
    "gamma = 0.15\n",
    "batch_size = 1\n",
    "\n",
    "losses, w = gradient_descent(all_data[:,0], all_data[:,1:], np.ones(X_standardized.shape[1]), max_iters, gamma)\n",
    "print(w)\n",
    "print(losses[-1])\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predicted = predict(X_standardized, w)\n",
    "Y_labeled = label_results(Y_predicted)\n",
    "print(performance(Y_labeled, label_results(Y)))\n",
    "# 0.735 for mse\n",
    "#0.74038 for cross-entropy\n",
    "evaluate_performance(label_results(Y), Y_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_threshold(Y_predicted, label_results(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.genfromtxt(testing_data, delimiter=',', skip_header=1)\n",
    "test_X = test_data[:, 2:]\n",
    "test_X_standardized = clean_and_standardize_features(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    test_predictions = label_results(predict(test_X_standardized, w))\n",
    "except:\n",
    "    test_predictions = label_results(predict(test_X_standardized, w[which]), 0.46)\n",
    "test_ids = range(350000,918238)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = np.column_stack([test_ids, test_predictions])\n",
    "suffix = time.time()\n",
    "np.savetxt('submission' + str(suffix) + '.csv', test_results, fmt=\"%d\", delimiter=\",\", header=\"Id,Prediction\", comments='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

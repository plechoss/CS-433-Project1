{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'data/'\n",
    "training_data = data_folder + 'train.csv'\n",
    "testing_data = data_folder + 'test.csv'\n",
    "\n",
    "methods = ['mse', 'mae', 'cross-enthropy']\n",
    "method = methods[2]\n",
    "\n",
    "pos_weight = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We map the 'b' and 's' labels to 0 and 1 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    Y = np.genfromtxt(training_data, delimiter=',', dtype=None, skip_header=1, usecols=[1], converters={1: lambda x: 0 if b'b'==x else 1})\n",
    "    \n",
    "    data = np.genfromtxt(training_data, delimiter=',', skip_header=1)\n",
    "    X = data[:, 2:]\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and standardize the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting the number of invalid datapoints per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 38114      0      0      0 177457 177457 177457      0      0      0\n",
      "      0      0 177457      0      0      0      0      0      0      0\n",
      "      0      0      0  99913  99913  99913 177457 177457 177457      0]\n"
     ]
    }
   ],
   "source": [
    "invalids = np.count_nonzero(X == -999, axis=0)\n",
    "print(invalids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore we get rid of columns [0,4,5,6,12,23,24,25,26,27,28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_standardize_features(X):\n",
    "    X_clean = np.delete(X,[0,4,5,6,12,23,24,25,26,27,28], axis=1)\n",
    "    X_standardized = (X_clean - X_clean.mean(axis=0))/X_clean.std(axis = 0)\n",
    "    X_standardized = np.insert(X_standardized, 0, 1, axis=1)\n",
    "    return X_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_standardized = clean_and_standardize_features(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods from lab1 and lab2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use the sigmoid function: $$S(z) = \\frac{1}{1 + e^{-z}}$$ <br />\n",
    "to map the predicted values to probabilities of the event being a signal(1) rather than background(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + math.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w):\n",
    "    temp = x@w\n",
    "    sigmoid_vec = np.vectorize(sigmoid)\n",
    "    return sigmoid_vec(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the cross-enthropy cost function for loss computation: \n",
    "$$J(\\theta) = -\\frac{1}{N} * (y^T log(Xw) + (1-y)^T log(1-Xw))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    predictions = tx@w\n",
    "    error = y-predictions\n",
    "    #print(error.shape)\n",
    "    #print((error*error).shape)\n",
    "    if(method == 'mse'):\n",
    "        return 1/(2*y.shape[0])*np.sum(error*error)\n",
    "    elif(method == 'mae'):\n",
    "        return 1/(2*y.shape[0])*np.sum(np.abs(error))\n",
    "    elif(method == 'cross-enthropy'):\n",
    "        predictions = predict(tx, w)\n",
    "        return np.sum(-y*np.log(predictions)*pos_weight + (1-y)*np.log(1-predictions))/y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(y, tx, w0, w1):\n",
    "    losses = np.zeros((len(w0), len(w1)))\n",
    "    for i in range(len(w0)):\n",
    "        for j in range(len(w1)):\n",
    "            losses[i][j] = compute_loss(y, tx, np.array([w0[i], w1[j]]))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    if(method=='cross-enthropy'):\n",
    "        prediction = predict(tx, w)\n",
    "    else:\n",
    "        prediction = tx@w\n",
    "    error = y-prediction\n",
    "    gradient = -1/y.shape[0]*tx.T@error\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    i = 0\n",
    "    w_res = initial_w\n",
    "    loss_hist = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        gradient = compute_gradient(y, tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        w = w - gamma * gradient\n",
    "        # store w and loss\n",
    "        w_res = w\n",
    "        loss_hist.append(loss)\n",
    "        #print(\"Gradient Descent({bi}/{ti}): loss={l}\".format(\n",
    "              #bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "        # Log Progress\n",
    "        i = i + 1\n",
    "        if i % 1000 == 0:\n",
    "            print(\"iter: \" + str(i) + \" loss: \"+str(loss_hist[-1]))\n",
    "\n",
    "    return loss_hist, w_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    w_res = initial_w\n",
    "    loss_res = 0\n",
    "    w = initial_w\n",
    "    ti = max_iters\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32, max_iters):\n",
    "        gradient = compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "        loss = compute_loss(minibatch_y, minibatch_tx, w)\n",
    "        w = w- gamma * gradient\n",
    "        # store w and loss\n",
    "        w_res = w\n",
    "        loss_res = loss\n",
    "    return loss_res, w_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We label the results <0.5 to -1, and the rest to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the threshold to 0.618 for mse, 0.458 for cross-enthropy\n",
    "def label_results(Y_predicted):\n",
    "    f = lambda x: -1 if x<0.458 else 1\n",
    "    f_vec = np.vectorize(f)\n",
    "    return f_vec(Y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(Y, Y_predicted):\n",
    "    return np.sum(Y == Y_predicted)/Y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance(Y, Y_predicted):\n",
    "    false_negatives = 0\n",
    "    true_negatives = 0\n",
    "    false_positives = 0\n",
    "    true_positives = 0\n",
    "    for i in range(Y.shape[0]):\n",
    "        if(Y[i]==-1):\n",
    "            if(Y_predicted[i]==-1):\n",
    "                true_negatives += 1\n",
    "            else:\n",
    "                false_positives += 1\n",
    "        else:\n",
    "            if(Y_predicted[i]==-1):\n",
    "                false_negatives += 1\n",
    "            else:\n",
    "                true_positives += 1\n",
    "    print('True positives: ' + str(true_positives))\n",
    "    print('False positives: ' + str(false_positives))\n",
    "    print('True negatives: ' + str(true_negatives))\n",
    "    print('False negatives: ' + str(false_negatives))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michal/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in log\n",
      "  if sys.path[0] == '':\n",
      "/Users/michal/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in multiply\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1000 loss: -0.09679397574456566\n",
      "[-8.73494804e-01 -6.82876112e-01 -8.98190458e-01  2.89303039e-01\n",
      "  9.73901777e-01 -1.27568761e-01 -5.66101388e-02 -5.43299412e-01\n",
      "  2.91476923e-01  6.02354370e-01 -3.19141320e-03 -2.50605404e-03\n",
      "  7.90827804e-01 -1.29274929e-03  6.50359620e-03  1.36025922e-01\n",
      "  4.31709331e-04 -1.97273737e-01  2.17392854e-01 -1.09299883e-01]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1180f2c88>]"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_iters = 1000\n",
    "gamma = 0.15\n",
    "batch_size = 1\n",
    "\n",
    "losses, w = gradient_descent(Y, X_standardized, np.ones(X_standardized.shape[1]), max_iters, gamma)\n",
    "print(w)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.740504\n"
     ]
    }
   ],
   "source": [
    "Y_predicted = predict(X_standardized, w)\n",
    "Y_labeled = label_results(Y_predicted)\n",
    "print(performance(Y_labeled, label_results(Y)))\n",
    "# 0.735 for mse\n",
    "#0.74038 for cross-enthropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives: 50600\n",
      "False positives: 29838\n",
      "True negatives: 134495\n",
      "False negatives: 35067\n"
     ]
    }
   ],
   "source": [
    "evaluate_performance(label_results(Y), Y_labeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oversampling the signal class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.genfromtxt(testing_data, delimiter=',', skip_header=1)\n",
    "test_X = test_data[:, 2:]\n",
    "test_X_standardized = clean_and_standardize_features(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = label_results(predict(test_X_standardized, w))\n",
    "test_ids = range(350000,918238)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = np.column_stack([test_ids, test_predictions])\n",
    "suffix = time.time()\n",
    "np.savetxt('submission' + str(suffix) + '.csv', test_results, fmt=\"%d\", delimiter=\",\", header=\"Id,Prediction\", comments='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

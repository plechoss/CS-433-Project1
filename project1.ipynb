{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'data/'\n",
    "training_data = data_folder + 'train.csv'\n",
    "testing_data = data_folder + 'test.csv'\n",
    "\n",
    "methods = ['mse', 'mae', 'cross-enthropy']\n",
    "method = methods[2]\n",
    "\n",
    "pos_weight = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We map the 'b' and 's' labels to 0 and 1 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    Y = np.genfromtxt(training_data, delimiter=',', dtype=None, skip_header=1, usecols=[1], converters={1: lambda x: 0 if b'b'==x else 1})\n",
    "    \n",
    "    data = np.genfromtxt(training_data, delimiter=',', skip_header=1)\n",
    "    X = data[:, 2:]\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and standardize the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting the number of invalid datapoints per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 38114      0      0      0 177457 177457 177457      0      0      0\n",
      "      0      0 177457      0      0      0      0      0      0      0\n",
      "      0      0      0  99913  99913  99913 177457 177457 177457      0]\n"
     ]
    }
   ],
   "source": [
    "invalids = np.count_nonzero(X == -999, axis=0)\n",
    "print(invalids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore we get rid of columns [0,4,5,6,12,23,24,25,26,27,28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_standardize_features(X):\n",
    "    X_clean = np.delete(X,[0,4,5,6,12,23,24,25,26,27,28], axis=1)\n",
    "    X_standardized = (X_clean - X_clean.mean(axis=0))/X_clean.std(axis = 0)\n",
    "    X_standardized = np.insert(X_standardized, 0, 1, axis=1)\n",
    "    return X_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_standardized = clean_and_standardize_features(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods from lab1 and lab2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use the sigmoid function: $$S(z) = \\frac{1}{1 + e^{-z}}$$ <br />\n",
    "to map the predicted values to probabilities of the event being a signal(1) rather than background(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + math.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w):\n",
    "    temp = x@w\n",
    "    sigmoid_vec = np.vectorize(sigmoid)\n",
    "    return sigmoid_vec(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the cross-enthropy cost function for loss computation: \n",
    "$$J(\\theta) = -\\frac{1}{N} * (y^T log(Xw) + (1-y)^T log(1-Xw))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    predictions = tx@w\n",
    "    error = y-predictions\n",
    "    #print(error.shape)\n",
    "    #print((error*error).shape)\n",
    "    if(method == 'mse'):\n",
    "        return 1/(2*y.shape[0])*np.sum(error*error)\n",
    "    elif(method == 'mae'):\n",
    "        return 1/(2*y.shape[0])*np.sum(np.abs(error))\n",
    "    elif(method == 'cross-enthropy'):\n",
    "        predictions = predict(tx, w)\n",
    "        return np.sum(-y*np.log(predictions)*pos_weight + (1-y)*np.log(1-predictions))/y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(y, tx, w0, w1):\n",
    "    losses = np.zeros((len(w0), len(w1)))\n",
    "    for i in range(len(w0)):\n",
    "        for j in range(len(w1)):\n",
    "            losses[i][j] = compute_loss(y, tx, np.array([w0[i], w1[j]]))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    if(method=='cross-enthropy'):\n",
    "        prediction = predict(tx, w)\n",
    "    else:\n",
    "        prediction = tx@w\n",
    "    error = y-prediction\n",
    "    gradient = -1/y.shape[0]*tx.T@error\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    i = 0\n",
    "    w_res = initial_w\n",
    "    loss_hist = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        gradient = compute_gradient(y, tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        w = w - gamma * gradient\n",
    "        # store w and loss\n",
    "        w_res = w\n",
    "        loss_hist.append(loss)\n",
    "        #print(\"Gradient Descent({bi}/{ti}): loss={l}\".format(\n",
    "              #bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "        # Log Progress\n",
    "        i = i + 1\n",
    "        if i % 1000 == 0:\n",
    "            print(\"iter: \" + str(i) + \" loss: \"+str(loss_hist[-1]))\n",
    "\n",
    "    return loss_hist, w_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    w_res = initial_w\n",
    "    loss_res = 0\n",
    "    w = initial_w\n",
    "    ti = max_iters\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32, max_iters):\n",
    "        gradient = compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "        loss = compute_loss(minibatch_y, minibatch_tx, w)\n",
    "        w = w- gamma * gradient\n",
    "        # store w and loss\n",
    "        w_res = w\n",
    "        loss_res = loss\n",
    "    return loss_res, w_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We label the results <0.5 to -1, and the rest to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_results(Y_predicted):\n",
    "    f = lambda x: -1 if x<0.45 else 1\n",
    "    f_vec = np.vectorize(f)\n",
    "    return f_vec(Y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(Y, Y_predicted):\n",
    "    return np.sum(Y == Y_predicted)/Y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance(Y, Y_predicted):\n",
    "    false_negatives = 0\n",
    "    true_negatives = 0\n",
    "    false_positives = 0\n",
    "    true_positives = 0\n",
    "    for i in range(Y.shape[0]):\n",
    "        if(Y[i]==-1):\n",
    "            if(Y_predicted[i]==-1):\n",
    "                true_negatives += 1\n",
    "            else:\n",
    "                false_positives += 1\n",
    "        else:\n",
    "            if(Y_predicted[i]==-1):\n",
    "                false_negatives += 1\n",
    "            else:\n",
    "                true_positives += 1\n",
    "    print('True positives: ' + str(true_positives))\n",
    "    print('False positives: ' + str(false_positives))\n",
    "    print('True negatives: ' + str(true_negatives))\n",
    "    print('False negatives: ' + str(false_negatives))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michal/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in log\n",
      "  if sys.path[0] == '':\n",
      "/Users/michal/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in multiply\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1000 loss: -0.09673866446704532\n",
      "[-8.60961081e-01 -6.81808971e-01 -8.05978086e-01  2.85158173e-01\n",
      "  9.10360057e-01 -1.25241808e-01 -5.41088236e-02 -3.96857555e-01\n",
      "  2.94290007e-01  6.52104574e-01 -3.39814457e-03 -3.01960479e-03\n",
      "  6.30770367e-01 -1.41739024e-03  6.38368261e-03  1.21734991e-01\n",
      "  5.03113942e-04 -1.98087741e-01  1.88232676e-01 -8.16915475e-02]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x114b6f400>]"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_iters = 1000\n",
    "gamma = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "losses, w = gradient_descent(Y, X_standardized, np.ones(X_standardized.shape[1]), max_iters, gamma)\n",
    "print(w)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.38654524 0.55960916 0.13541126 ... 0.27395135 0.65615291 0.14825504]\n",
      "[-1  1 -1 -1 -1 -1  1  1 -1 -1 -1  1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1  1\n",
      " -1 -1  1 -1  1  1 -1  1  1 -1 -1 -1  1 -1 -1  1]\n"
     ]
    }
   ],
   "source": [
    "Y_predicted = predict(X_standardized, w)\n",
    "print(Y_predicted)\n",
    "Y_labeled = label_results(Y_predicted)\n",
    "print(Y_labeled[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.74006\n"
     ]
    }
   ],
   "source": [
    "print(performance(Y_labeled, label_results(Y)))\n",
    "# 0.735 for mse\n",
    "#0.74018 for cross-enthropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives: 51060\n",
      "False positives: 30348\n",
      "True negatives: 133985\n",
      "False negatives: 34607\n"
     ]
    }
   ],
   "source": [
    "evaluate_performance(label_results(Y), Y_labeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.genfromtxt(testing_data, delimiter=',', skip_header=1)\n",
    "test_X = test_data[:, 2:]\n",
    "test_X_standardized = clean_and_standardize_features(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = label_results(predict(test_X_standardized, w))\n",
    "test_ids = range(350000,918238)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = np.column_stack([test_ids, test_predictions])\n",
    "suffix = time.time()\n",
    "np.savetxt('submission' + str(suffix) + '.csv', test_results, fmt=\"%d\", delimiter=\",\", header=\"Id,Prediction\", comments='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

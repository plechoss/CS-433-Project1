{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from global_variables import *\n",
    "from data_preparation import * \n",
    "from cost import * \n",
    "from cross_validation import *\n",
    "from performances import * \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We map the 'b' and 's' labels to 0 and 1 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and standardize the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting the number of invalid datapoints per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "invalids = np.count_nonzero(X == -999, axis=0)\n",
    "print(invalids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore we get rid of columns [0,4,5,6,12,23,24,25,26,27,28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_standardized = clean_and_standardize_features(X)\n",
    "N = X_standardized.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = np.corrcoef(X_standardized[:10000,:], rowvar=False)\n",
    "suffix = time.time()\n",
    "np.savetxt('correlation_matrix_' + str(suffix) + '.csv', correlation_matrix, fmt=\"%0.2f\", delimiter=\",\", comments='')\n",
    "correlation_heatmap(X_standardized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods from lab1 and lab2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use the sigmoid function: $$S(z) = \\frac{1}{1 + e^{-z}}$$ <br />\n",
    "to map the predicted values to probabilities of the event being a signal(1) rather than background(0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the cross-entropy cost function for loss computation: \n",
    "$$J(\\theta) = -\\frac{1}{N} * (y^T log(Xw) + (1-y)^T log(1-Xw))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(y, tx, w0, w1):\n",
    "    losses = np.zeros((len(w0), len(w1)))\n",
    "    for i in range(len(w0)):\n",
    "        for j in range(len(w1)):\n",
    "            losses[i][j] = compute_loss(y, tx, np.array([w0[i], w1[j]]))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We label the results <0.5 to -1, and the rest to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying the $L_2$ - regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_prime = lambda_1 * 2 * N\n",
    "temp = np.linalg.inv(X_standardized.T@X_standardized + np.identity(N)*lambda_prime)\n",
    "temp_2 = temp@X_standardized.T\n",
    "w_L_2 = temp_2@Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predicted = predict(X_standardized, w_L_2)\n",
    "Y_labeled = label_results(Y_predicted)\n",
    "print(performance(Y_labeled, label_results(Y)))\n",
    "# 0.735 for mse\n",
    "#0.74038 for cross-entropy\n",
    "evaluate_performance(label_results(Y), Y_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_threshold(Y_predicted, label_results(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying the ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 1000\n",
    "gamma = 0.15\n",
    "batch_size = 1\n",
    "\n",
    "method = methods[4]\n",
    "losses, w_ridge = gradient_descent(Y, X_standardized, np.ones(X_standardized.shape[1]), max_iters, gamma, method, lambda_1)\n",
    "print(w_ridge)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predicted = predict(X_standardized, w_ridge)\n",
    "Y_labeled = label_results(Y_predicted)\n",
    "print(performance(Y_labeled, label_results(Y)))\n",
    "# 0.735 for mse\n",
    "#0.74038 for cross-entropy\n",
    "evaluate_performance(label_results(Y), Y_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_threshold(Y_predicted, label_results(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 100\n",
    "gamma = 0.01\n",
    "batch_size = 1\n",
    "\n",
    "method = methods[2]\n",
    "losses, w = gradient_descent(Y, X_standardized, np.random.random(X_standardized.shape[1]), max_iters, gamma, method)\n",
    "print(w)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare different values of gamma for gradient descent\n",
    "max_iters = 600\n",
    "gamma = [0.5, 0.1, 0.15, 0.2, 0.3, 1]\n",
    "batch_size = 1\n",
    "whichLoss = 100\n",
    "which = 100\n",
    "losses = np.ndarray([len(gamma),1])\n",
    "w = np.ndarray([len(gamma), X_standardized.shape[1]])\n",
    "for i in range(len(gamma)):\n",
    "    loss, wi = gradient_descent(Y, X_standardized, np.random.random(X_standardized.shape[1]), max_iters, gamma[i]) \n",
    "    plt.tight_layout()\n",
    "    plt.subplot(int(str(32)+str(i+1)))\n",
    "    plt.plot(loss)\n",
    "    plt.xlabel(\"# of iterations\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.title(\"Gamma = \" + str(gamma[i]))\n",
    "    print(loss[-1])\n",
    "    if(loss[-1] < whichLoss):\n",
    "        whichLoss = loss[-1]\n",
    "        which = i\n",
    "    losses[i] = loss[-1]\n",
    "    w[i,:] = wi\n",
    "\n",
    "print(\"smallest cost:\" + str(losses[which]) + \"at gamma=\" + str(gamma[which]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    Y_predicted = predict(X_standardized, w)\n",
    "except:\n",
    "    Y_predicted = predict(X_standardized, w[which,:])\n",
    "Y_labeled = label_results(Y_predicted, 0.46)\n",
    "print(performance(Y_labeled, label_results(Y)))\n",
    "# 0.735 for mse\n",
    "#0.74038 for cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_performance(label_results(Y), Y_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_threshold(Y_predicted, label_results(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing cost functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "least-squares\n",
      "[0.08812818]\n",
      "least-squares-GD\n",
      "[0.31149537]\n",
      "least-squares-SGD\n",
      "[0.10452225]\n",
      "log\n",
      "difference to last loss 0.0011118422591115396\n",
      "difference to last loss 0.0010969211654093813\n",
      "difference to last loss 0.0010823049197025147\n",
      "difference to last loss 0.0010679863784768262\n",
      "difference to last loss 0.0010539585379748528\n",
      "difference to last loss 0.001040214534735573\n",
      "difference to last loss 0.0010267476458332148\n",
      "difference to last loss 0.0010135512888308496\n",
      "difference to last loss 0.0010006190214739696\n",
      "difference to last loss 0.0009879445411310472\n",
      "difference to last loss 0.0009755216840204861\n",
      "difference to last loss 0.0009633444242236333\n",
      "difference to last loss 0.0009514068725164915\n",
      "difference to last loss 0.0009397032750247947\n",
      "difference to last loss 0.000928228011733645\n",
      "difference to last loss 0.0009169755948528202\n",
      "difference to last loss 0.0009059406670626213\n",
      "difference to last loss 0.0008951179996534719\n",
      "difference to last loss 0.0008845024905645982\n",
      "difference to last loss 0.0008740891623484348\n",
      "difference to last loss 0.0008638731600596472\n",
      "difference to last loss 0.000853849749083313\n",
      "difference to last loss 0.0008440143129223587\n",
      "difference to last loss 0.0008343623509334819\n",
      "difference to last loss 0.0008248894760440884\n",
      "difference to last loss 0.0008155914124365893\n",
      "difference to last loss 0.0008064639932250373\n",
      "difference to last loss 0.0007975031581195502\n",
      "difference to last loss 0.0007887049510881816\n",
      "difference to last loss 0.000780065518023898\n",
      "difference to last loss 0.0007715811044186616\n",
      "difference to last loss 0.0007632480530516128\n",
      "difference to last loss 0.0007550628016953498\n",
      "difference to last loss 0.0007470218808381945\n",
      "difference to last loss 0.0007391219114392111\n",
      "difference to last loss 0.00073135960270021\n",
      "difference to last loss 0.0007237317498760554\n",
      "difference to last loss 0.0007162352321128385\n",
      "difference to last loss 0.0007088670103180261\n",
      "difference to last loss 0.0007016241250709099\n",
      "difference to last loss 0.0006945036945663619\n",
      "difference to last loss 0.0006875029125951171\n",
      "difference to last loss 0.0006806190465694639\n",
      "difference to last loss 0.0006738494355802427\n",
      "difference to last loss 0.0006671914884996966\n",
      "difference to last loss 0.0006606426821249567\n",
      "difference to last loss 0.0006542005593564992\n",
      "difference to last loss 0.0006478627274268955\n",
      "difference to last loss 0.0006416268561603156\n",
      "difference to last loss 0.0006354906762834345\n",
      "difference to last loss 0.0006294519777669816\n",
      "difference to last loss 0.0006235086082166941\n",
      "difference to last loss 0.000617658471295246\n",
      "difference to last loss 0.0006118995251944703\n",
      "difference to last loss 0.000606229781136336\n",
      "difference to last loss 0.0006006473019187775\n",
      "difference to last loss 0.0005951502005004938\n",
      "difference to last loss 0.0005897366386157232\n",
      "difference to last loss 0.0005844048254359802\n",
      "difference to last loss 0.0005791530162599923\n",
      "difference to last loss 0.0005739795112413848\n",
      "difference to last loss 0.0005688826541534464\n",
      "difference to last loss 0.0005638608311838711\n",
      "difference to last loss 0.0005589124697654713\n",
      "difference to last loss 0.0005540360374383102\n",
      "difference to last loss 0.0005492300407431427\n",
      "difference to last loss 0.0005444930241453871\n",
      "difference to last loss 0.0005398235689898501\n",
      "difference to last loss 0.0005352202924849836\n",
      "difference to last loss 0.0005306818467143426\n",
      "difference to last loss 0.0005262069176754647\n",
      "difference to last loss 0.000521794224348171\n",
      "difference to last loss 0.0005174425177852937\n",
      "difference to last loss 0.0005131505802312697\n",
      "difference to last loss 0.0005089172242652706\n",
      "difference to last loss 0.000504741291965316\n",
      "difference to last loss 0.000500621654101252\n",
      "difference to last loss 0.0004965572093424964\n",
      "difference to last loss 0.0004925468834933167\n",
      "difference to last loss 0.0004885896287490921\n",
      "difference to last loss 0.0004846844229701164\n",
      "difference to last loss 0.00048083026897471903\n",
      "difference to last loss 0.0004770261938562559\n",
      "difference to last loss 0.0004732712483153101\n",
      "difference to last loss 0.0004695645060096565\n",
      "difference to last loss 0.000465905062923877\n",
      "difference to last loss 0.00046229203675618447\n",
      "difference to last loss 0.0004587245663177919\n",
      "difference to last loss 0.0004552018109564848\n",
      "difference to last loss 0.00045172294998574447\n",
      "difference to last loss 0.00044828718213951735\n",
      "difference to last loss 0.0004448937250347562\n",
      "difference to last loss 0.00044154181465183573\n",
      "difference to last loss 0.00043823070482740256\n",
      "difference to last loss 0.0004349596667658773\n",
      "difference to last loss 0.00043172798855872774\n",
      "difference to last loss 0.00042853497471750934\n",
      "difference to last loss 0.00042537994572855453\n",
      "difference to last loss 0.0004222622376098828\n",
      "difference to last loss 0.00041918120148387583\n",
      "difference to last loss 0.0004161362031684934\n",
      "difference to last loss 0.0004131266227713759\n",
      "difference to last loss 0.0004101518543025984\n",
      "difference to last loss 0.0004072113052935311\n",
      "difference to last loss 0.00040430439643457383\n",
      "difference to last loss 0.00040143056121466625\n",
      "difference to last loss 0.0003985892455798945\n",
      "difference to last loss 0.0003957799075960944\n",
      "difference to last loss 0.000393002017128663\n",
      "difference to last loss 0.00039025505552403583\n",
      "difference to last loss 0.00038753851531048156\n",
      "difference to last loss 0.00038485189989856394\n",
      "difference to last loss 0.00038219472329947823\n",
      "difference to last loss 0.0003795665098473844\n",
      "difference to last loss 0.00037696679393117716\n",
      "difference to last loss 0.0003743951197369144\n",
      "difference to last loss 0.000371851040993576\n",
      "difference to last loss 0.0003693341207359202\n",
      "difference to last loss 0.00036684393106189983\n",
      "difference to last loss 0.0003643800529101737\n",
      "difference to last loss 0.0003619420758359526\n",
      "difference to last loss 0.00035952959779883553\n",
      "difference to last loss 0.0003571422249543099\n",
      "difference to last loss 0.0003547795714520241\n",
      "difference to last loss 0.00035244125924338565\n",
      "difference to last loss 0.00035012691788838257\n",
      "difference to last loss 0.00034783618437661534\n",
      "difference to last loss 0.0003455687029449983\n",
      "difference to last loss 0.00034332412490889475\n",
      "difference to last loss 0.00034110210849025435\n",
      "difference to last loss 0.0003389023186577411\n",
      "difference to last loss 0.0003367244269678604\n",
      "difference to last loss 0.0003345681114086396\n",
      "difference to last loss 0.00033243305625374475\n",
      "difference to last loss 0.0003303189519129335\n",
      "difference to last loss 0.0003282254947946095\n",
      "difference to last loss 0.0003261523871632699\n",
      "difference to last loss 0.00032409933701116334\n",
      "difference to last loss 0.00032206605792184373\n",
      "difference to last loss 0.0003200522689470464\n",
      "difference to last loss 0.00031805769448145504\n",
      "difference to last loss 0.00031608206414202034\n",
      "difference to last loss 0.0003141251126497213\n",
      "difference to last loss 0.00031218657971665564\n",
      "difference to last loss 0.00031026620993368503\n",
      "difference to last loss 0.0003083637526585248\n",
      "difference to last loss 0.0003064789619148245\n",
      "difference to last loss 0.00030461159628403234\n",
      "difference to last loss 0.00030276141880669627\n",
      "difference to last loss 0.0003009281968819888\n",
      "difference to last loss 0.0002991117021740042\n",
      "difference to last loss 0.00029731171051483596\n",
      "difference to last loss 0.00029552800181598116\n",
      "difference to last loss 0.00029376035997552563\n",
      "difference to last loss 0.0002920085727941002\n",
      "difference to last loss 0.0002902724318881722\n",
      "difference to last loss 0.0002885517326048914\n",
      "difference to last loss 0.00028684627394626183\n",
      "difference to last loss 0.00028515585848376546\n",
      "difference to last loss 0.00028348029228453253\n",
      "difference to last loss 0.0002818193848355133\n",
      "difference to last loss 0.00028017294896898193\n",
      "difference to last loss 0.0002785408007900392\n",
      "difference to last loss 0.00027692275960833346\n",
      "difference to last loss 0.0002753186478667846\n",
      "difference to last loss 0.00027372829107752406\n",
      "difference to last loss 0.0002721515177525058\n",
      "difference to last loss 0.0002705881593449977\n",
      "difference to last loss 0.00026903805018108073\n",
      "difference to last loss 0.00026750102740469295\n",
      "difference to last loss 0.0002659769309136806\n",
      "difference to last loss 0.00026446560330350977\n",
      "difference to last loss 0.0002629668898107562\n",
      "difference to last loss 0.0002614806382579271\n",
      "difference to last loss 0.00026000669899906015\n",
      "difference to last loss 0.00025854492486476754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference to last loss 0.0002570951711158287\n",
      "difference to last loss 0.0002556572953885672\n",
      "difference to last loss 0.00025423115764577897\n",
      "difference to last loss 0.00025281662013199035\n",
      "difference to last loss 0.00025141354732338694\n",
      "difference to last loss 0.0002500218058838488\n",
      "difference to last loss 0.0002486412646189873\n",
      "difference to last loss 0.0002472717944342895\n",
      "difference to last loss 0.0002459132682902654\n",
      "difference to last loss 0.00024456556116247974\n",
      "difference to last loss 0.000243228550001251\n",
      "difference to last loss 0.00024190211368979586\n",
      "difference to last loss 0.00024058613300736997\n",
      "difference to last loss 0.00023928049059063206\n",
      "difference to last loss 0.00023798507089722865\n",
      "difference to last loss 0.00023669976016937877\n",
      "difference to last loss 0.00023542444639745863\n",
      "difference to last loss 0.00023415901928747207\n",
      "difference to last loss 0.00023290337022596752\n",
      "difference to last loss 0.00023165739224795256\n",
      "difference to last loss 0.00023042098000369826\n",
      "difference to last loss 0.00022919402972831904\n",
      "difference to last loss 0.0002279764392110195\n",
      "difference to last loss 0.00022676810776467438\n",
      "difference to last loss 0.0002255689361958524\n",
      "difference to last loss 0.00022437882677794896\n",
      "difference to last loss 0.0002231976832219873\n",
      "difference to last loss 0.0002220254106491959\n",
      "difference to last loss 0.00022086191556414114\n",
      "difference to last loss 0.00021970710583041342\n",
      "difference to last loss 0.00021856089064209439\n",
      "difference to last loss 0.00021742318050077536\n",
      "difference to last loss 0.00021629388719179854\n",
      "difference to last loss 0.00021517292375705654\n",
      "difference to last loss 0.0002140602044764517\n",
      "difference to last loss 0.00021295564484002938\n",
      "difference to last loss 0.00021185916152965945\n",
      "difference to last loss 0.0002107706723959435\n",
      "difference to last loss 0.00020969009643501124\n",
      "difference to last loss 0.00020861735377120105\n",
      "difference to last loss 0.00020755236563263502\n",
      "difference to last loss 0.0002064950543350097\n",
      "difference to last loss 0.00020544534325961372\n",
      "difference to last loss 0.00020440315683445398\n",
      "difference to last loss 0.00020336842051804638\n",
      "difference to last loss 0.00020234106077610114\n",
      "difference to last loss 0.00020132100507008754\n",
      "difference to last loss 0.00020030818183269794\n",
      "difference to last loss 0.00019930252045829988\n",
      "difference to last loss 0.00019830395127817813\n",
      "difference to last loss 0.0001973124055513198\n",
      "difference to last loss 0.0001963278154437642\n",
      "difference to last loss 0.00019535011401483615\n",
      "difference to last loss 0.00019437923520027045\n",
      "difference to last loss 0.00019341511379911136\n",
      "difference to last loss 0.00019245768545772535\n",
      "difference to last loss 0.00019150688665525717\n",
      "difference to last loss 0.0001905626546903072\n",
      "difference to last loss 0.00018962492766649852\n",
      "difference to last loss 0.00018869364447859915\n",
      "difference to last loss 0.00018776874479997652\n",
      "difference to last loss 0.00018685016906971885\n",
      "difference to last loss 0.00018593785847897948\n",
      "difference to last loss 0.00018503175495965252\n",
      "difference to last loss 0.00018413180116971795\n",
      "difference to last loss 0.00018323794048558106\n",
      "difference to last loss 0.00018235011698641834\n",
      "difference to last loss 0.00018146827544429645\n",
      "difference to last loss 0.00018059236131240386\n",
      "difference to last loss 0.00017972232071528094\n",
      "difference to last loss 0.00017885810043571926\n",
      "difference to last loss 0.00017799964790743417\n",
      "difference to last loss 0.00017714691120052084\n",
      "difference to last loss 0.00017629983901523705\n",
      "difference to last loss 0.00017545838066945763\n",
      "difference to last loss 0.00017462248609001474\n",
      "difference to last loss 0.00017379210580270588\n",
      "difference to last loss 0.00017296719092374513\n",
      "difference to last loss 0.00017214769314821687\n",
      "difference to last loss 0.00017133356474507977\n",
      "difference to last loss 0.0001705247585425118\n",
      "difference to last loss 0.00016972122792646704\n",
      "difference to last loss 0.00016892292682513244\n",
      "difference to last loss 0.00016812980970482005\n",
      "difference to last loss 0.00016734183156241755\n",
      "difference to last loss 0.00016655894791173242\n",
      "difference to last loss 0.00016578111478338098\n",
      "difference to last loss 0.00016500828870991135\n",
      "difference to last loss 0.0001642404267242492\n",
      "difference to last loss 0.00016347748634704118\n",
      "difference to last loss 0.00016271942558321317\n",
      "difference to last loss 0.00016196620291242247\n",
      "difference to last loss 0.0001612177772839507\n",
      "difference to last loss 0.00016047410810726692\n",
      "difference to last loss 0.00015973515524803084\n",
      "difference to last loss 0.00015900087901876692\n",
      "difference to last loss 0.00015827124017520067\n",
      "difference to last loss 0.00015754619990748786\n",
      "difference to last loss 0.00015682571983433036\n",
      "difference to last loss 0.00015610976199820215\n",
      "difference to last loss 0.00015539828885724472\n",
      "difference to last loss 0.0001546912632810482\n",
      "difference to last loss 0.00015398864854376804\n",
      "difference to last loss 0.0001532904083187958\n",
      "difference to last loss 0.00015259650667287517\n",
      "difference to last loss 0.00015190690806077267\n",
      "difference to last loss 0.00015122157731961572\n",
      "difference to last loss 0.0001505404796638965\n",
      "difference to last loss 0.0001498635806794768\n",
      "difference to last loss 0.00014919084631936919\n",
      "difference to last loss 0.00014852224289829685\n",
      "difference to last loss 0.00014785773708725358\n",
      "difference to last loss 0.00014719729590850772\n",
      "difference to last loss 0.00014654088673293764\n",
      "difference to last loss 0.00014588847727114995\n",
      "difference to last loss 0.00014524003557359055\n",
      "difference to last loss 0.0001445955300221069\n",
      "difference to last loss 0.00014395492932806064\n",
      "difference to last loss 0.00014331820252577732\n",
      "difference to last loss 0.00014268531896999281\n",
      "difference to last loss 0.0001420562483314125\n",
      "difference to last loss 0.00014143096059027194\n",
      "difference to last loss 0.0001408094260356707\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot copy sequence with size 300 to array axis with dimension 20",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-daa132c7ed9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod_i\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethods2compare\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mML_methods\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot copy sequence with size 300 to array axis with dimension 20"
     ]
    }
   ],
   "source": [
    "# call up the different methods to find minimize the error to the dataset\n",
    "# store the corresponding losses and compare them in a table\n",
    "# need to define y, tx, initial_w, batch_size, max_iters, gamma, lambda_r\n",
    "#x, y = load_data()\n",
    "#tx = clean_and_standardize_features(x)\n",
    "\n",
    "# initialize parameters\n",
    "initial_w = np.random.rand(tx.shape[1])\n",
    "max_iters = 300\n",
    "gamma = 0.01\n",
    "batch_size = 1\n",
    "lambda_ = 1\n",
    "\n",
    "methods2compare = ['least-squares', 'least-squares-GD', 'least-squares-SGD', 'log', 'regularized-log'] \n",
    "losses = np.ndarray([len(methods2compare),1])\n",
    "weights = np.ndarray([len(methods2compare),tx.shape[1]])\n",
    "\n",
    "for ind, method_i in enumerate(methods2compare):\n",
    "    print(method_i)\n",
    "    weights[ind,:], losses[ind] = ML_methods(y, tx, method_i, initial_w, batch_size, max_iters, gamma, lambda_)\n",
    "    print(losses[ind])\n",
    "    \n",
    "plt.bar(np.arange(len(losses)), losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.log(1+math.exp(predictions)) - y@predictions.T)/y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oversampling the signal class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = np.column_stack([Y, X_standardized])\n",
    "distrib = np.bincount(all_data[:,0].astype(int))\n",
    "prob = 1/distrib[all_data[:, 0].astype(int)].astype(float)\n",
    "prob /= prob.sum()\n",
    "all_data = all_data[np.random.choice(np.arange(len(all_data)), size=np.count_nonzero(distrib)*distrib.max(), p=prob)]\n",
    "print(all_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rerunning the gradient descent with oversampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 500\n",
    "gamma = 0.15\n",
    "batch_size = 1\n",
    "\n",
    "losses, w = gradient_descent(all_data[:,0], all_data[:,1:], np.ones(X_standardized.shape[1]), max_iters, gamma, method)\n",
    "print(w)\n",
    "print(losses[-1])\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predicted = predict(X_standardized, w)\n",
    "Y_labeled = label_results(Y_predicted)\n",
    "print(performance(Y_labeled, label_results(Y)))\n",
    "# 0.735 for mse\n",
    "#0.74038 for cross-enthropy\n",
    "evaluate_performance(label_results(Y), Y_labeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.genfromtxt(testing_data, delimiter=',', skip_header=1)\n",
    "test_X = test_data[:, 2:]\n",
    "test_X_standardized = clean_and_standardize_features(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = label_results(predict(test_X_standardized, w))\n",
    "test_ids = range(350000,918238)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = np.column_stack([test_ids, test_predictions])\n",
    "suffix = time.time()\n",
    "np.savetxt('submission' + str(suffix) + '.csv', test_results, fmt=\"%d\", delimiter=\",\", header=\"Id,Prediction\", comments='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

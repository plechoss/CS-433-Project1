{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (cross_validation.py, line 33)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\LMD\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m3325\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-b7bf7aa44285>\"\u001b[1;36m, line \u001b[1;32m4\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    from cross_validation import *\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\LMD\\Desktop\\CS-433-Project1\\scripts\\cross_validation.py\"\u001b[1;36m, line \u001b[1;32m33\u001b[0m\n\u001b[1;33m    x_tr_clean = clean_features(x_tr, clean_method):\u001b[0m\n\u001b[1;37m                                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from global_variables import *\n",
    "from data_preparation import * \n",
    "from cost import * \n",
    "from cross_validation import *\n",
    "from performances import * \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We map the 'b' and 's' labels to 0 and 1 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and standardize the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting the number of invalid datapoints per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "invalids = np.count_nonzero(X == -999, axis=0)\n",
    "print(invalids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore we get rid of columns [0,4,5,6,12,23,24,25,26,27,28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_standardized = clean_and_standardize_features(X)\n",
    "N = X_standardized.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = np.corrcoef(X_standardized[:10000,:], rowvar=False)\n",
    "suffix = time.time()\n",
    "np.savetxt('correlation_matrix_' + str(suffix) + '.csv', correlation_matrix, fmt=\"%0.2f\", delimiter=\",\", comments='')\n",
    "correlation_heatmap(X_standardized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods from lab1 and lab2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use the sigmoid function: $$S(z) = \\frac{1}{1 + e^{-z}}$$ <br />\n",
    "to map the predicted values to probabilities of the event being a signal(1) rather than background(0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the cross-entropy cost function for loss computation: \n",
    "$$J(\\theta) = -\\frac{1}{N} * (y^T log(Xw) + (1-y)^T log(1-Xw))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(y, tx, w0, w1):\n",
    "    losses = np.zeros((len(w0), len(w1)))\n",
    "    for i in range(len(w0)):\n",
    "        for j in range(len(w1)):\n",
    "            losses[i][j] = compute_loss(y, tx, np.array([w0[i], w1[j]]))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We label the results <0.5 to -1, and the rest to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying the $L_2$ - regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_prime = lambda_1 * 2 * N\n",
    "temp = np.linalg.inv(X_standardized.T@X_standardized + np.identity(N)*lambda_prime)\n",
    "temp_2 = temp@X_standardized.T\n",
    "w_L_2 = temp_2@Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predicted = predict(X_standardized, w_L_2)\n",
    "Y_labeled = label_results(Y_predicted)\n",
    "print(performance(Y_labeled, label_results(Y)))\n",
    "# 0.735 for mse\n",
    "#0.74038 for cross-entropy\n",
    "evaluate_performance(label_results(Y), Y_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_threshold(Y_predicted, label_results(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying the ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 1000\n",
    "gamma = 0.15\n",
    "batch_size = 1\n",
    "\n",
    "method = methods[4]\n",
    "losses, w_ridge = gradient_descent(Y, X_standardized, np.ones(X_standardized.shape[1]), max_iters, gamma, method, lambda_1)\n",
    "print(w_ridge)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predicted = predict(X_standardized, w_ridge)\n",
    "Y_labeled = label_results(Y_predicted)\n",
    "print(performance(Y_labeled, label_results(Y)))\n",
    "# 0.735 for mse\n",
    "#0.74038 for cross-entropy\n",
    "evaluate_performance(label_results(Y), Y_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_threshold(Y_predicted, label_results(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 100\n",
    "gamma = 0.01\n",
    "batch_size = 1\n",
    "\n",
    "method = methods[2]\n",
    "losses, w = gradient_descent(Y, X_standardized, np.random.random(X_standardized.shape[1]), max_iters, gamma, method)\n",
    "print(w)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare different values of gamma for gradient descent\n",
    "max_iters = 600\n",
    "gamma = [0.5, 0.1, 0.15, 0.2, 0.3, 1]\n",
    "batch_size = 1\n",
    "whichLoss = 100\n",
    "which = 100\n",
    "losses = np.ndarray([len(gamma),1])\n",
    "w = np.ndarray([len(gamma), X_standardized.shape[1]])\n",
    "for i in range(len(gamma)):\n",
    "    loss, wi = gradient_descent(Y, X_standardized, np.random.random(X_standardized.shape[1]), max_iters, gamma[i]) \n",
    "    plt.tight_layout()\n",
    "    plt.subplot(int(str(32)+str(i+1)))\n",
    "    plt.plot(loss)\n",
    "    plt.xlabel(\"# of iterations\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.title(\"Gamma = \" + str(gamma[i]))\n",
    "    print(loss[-1])\n",
    "    if(loss[-1] < whichLoss):\n",
    "        whichLoss = loss[-1]\n",
    "        which = i\n",
    "    losses[i] = loss[-1]\n",
    "    w[i,:] = wi\n",
    "\n",
    "print(\"smallest cost:\" + str(losses[which]) + \"at gamma=\" + str(gamma[which]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    Y_predicted = predict(X_standardized, w)\n",
    "except:\n",
    "    Y_predicted = predict(X_standardized, w[which,:])\n",
    "Y_labeled = label_results(Y_predicted, 0.46)\n",
    "print(performance(Y_labeled, label_results(Y)))\n",
    "# 0.735 for mse\n",
    "#0.74038 for cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_performance(label_results(Y), Y_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_threshold(Y_predicted, label_results(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing cost functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_and_standardize_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-774f4a981aeb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_and_standardize_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# initialize parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clean_and_standardize_features' is not defined"
     ]
    }
   ],
   "source": [
    "# call up the different methods to find minimize the error to the dataset\n",
    "# store the corresponding losses and compare them in a table\n",
    "# need to define y, tx, initial_w, batch_size, max_iters, gamma, lambda_r\n",
    "\n",
    "#x, y = load_data()\n",
    "tx = clean_and_standardize_features(x)\n",
    "\n",
    "# initialize parameters\n",
    "initial_w = np.random.rand(tx.shape[1])\n",
    "max_iters = 30\n",
    "gamma = 0.001\n",
    "batch_size = 1\n",
    "lambda_ = 1\n",
    "\n",
    "methods2compare = ['least-squares', 'least-squares-GD', 'least-squares-SGD', 'ridge-regression', 'log', 'regularized-log', 'log-newton'] \n",
    "losses = np.ndarray([len(methods2compare),1])\n",
    "weights = np.ndarray([len(methods2compare),tx.shape[1]])\n",
    "\n",
    "for ind, method_i in enumerate(methods2compare):\n",
    "    print(method_i)\n",
    "    weights[ind,:], losses[ind] = ML_methods(y, tx, method_i, initial_w, batch_size, max_iters, gamma, lambda_)\n",
    "    print(\"Cost: \" + str(losses[ind]))\n",
    "\n",
    "# Bring cost to list for plotting \n",
    "barY = list()\n",
    "for i in range(len(losses)):\n",
    "    barY.append(losses[i,0].tolist())\n",
    "\n",
    "# Create bar plot with labels\n",
    "plt.bar(np.arange(len(losses)), barY)\n",
    "plt.xticks(np.arange(len(losses)), methods2compare, rotation='60')\n",
    "plt.ylabel(\"Evaluated Cost\")\n",
    "plt.title(\"Cost Functions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n",
      "(9,)\n"
     ]
    }
   ],
   "source": [
    "#x, y = load_data()\n",
    "#mu, sigma = findMeanSigma(x)\n",
    "#xStd = (x-mu)/sigma\n",
    "#removed = features_to_remove(xStd, y)\n",
    "print(xStd.shape)\n",
    "x_new = np.delete(xStd, removed, 1)\n",
    "print(removed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oversampling the signal class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = np.column_stack([Y, X_standardized])\n",
    "distrib = np.bincount(all_data[:,0].astype(int))\n",
    "prob = 1/distrib[all_data[:, 0].astype(int)].astype(float)\n",
    "prob /= prob.sum()\n",
    "all_data = all_data[np.random.choice(np.arange(len(all_data)), size=np.count_nonzero(distrib)*distrib.max(), p=prob)]\n",
    "print(all_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rerunning the gradient descent with oversampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 500\n",
    "gamma = 0.15\n",
    "batch_size = 1\n",
    "\n",
    "losses, w = gradient_descent(all_data[:,0], all_data[:,1:], np.ones(X_standardized.shape[1]), max_iters, gamma, method)\n",
    "print(w)\n",
    "print(losses[-1])\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predicted = predict(X_standardized, w)\n",
    "Y_labeled = label_results(Y_predicted)\n",
    "print(performance(Y_labeled, label_results(Y)))\n",
    "# 0.735 for mse\n",
    "#0.74038 for cross-enthropy\n",
    "evaluate_performance(label_results(Y), Y_labeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.genfromtxt(testing_data, delimiter=',', skip_header=1)\n",
    "test_X = test_data[:, 2:]\n",
    "test_X_standardized = clean_and_standardize_features(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = label_results(predict(test_X_standardized, w))\n",
    "test_ids = range(350000,918238)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = np.column_stack([test_ids, test_predictions])\n",
    "suffix = time.time()\n",
    "np.savetxt('submission' + str(suffix) + '.csv', test_results, fmt=\"%d\", delimiter=\",\", header=\"Id,Prediction\", comments='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
